{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import cv2                   # to capture image\n","import mediapipe as mp       # to capture hand and use mutiple functions and classes related to it\n","import math\n","import numpy as np"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["from ctypes import cast, POINTER\n","from comtypes import CLSCTX_ALL\n","from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["wCam, hcam= 640, 480"]},{"cell_type":"markdown","metadata":{},"source":["to access speakers through library pycaw"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["devices = AudioUtilities.GetSpeakers()\n","interface = devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)\n","volume = cast(interface, POINTER(IAudioEndpointVolume))\n","#print(volume.GetMute(),\"\\n\")\n","#print(volume.GetMasterVolumeLevel(),\"\\n\")\n","#print(volume.GetVolumeRange())"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["mp_hands = mp.solutions.hands                        # recognize hands in the image\n","hands=mp_hands.Hands()                               # configuration of hand\n","capture = cv2.VideoCapture(0)                        #webcam input\n","#mp_drawing = mp.solutions.drawing_utils"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"ename":"Error","evalue":"Canceled future for execute_request message before replies were done","output_type":"error","traceback":["Error: Canceled future for execute_request message before replies were done","at t.KernelShellFutureHandler.dispose (c:\\Users\\hp\\.vscode\\extensions\\ms-toolsai.jupyter-2022.4.1000912146\\out\\extension.node.js:2:1199393)","at c:\\Users\\hp\\.vscode\\extensions\\ms-toolsai.jupyter-2022.4.1000912146\\out\\extension.node.js:2:1218445","at Map.forEach (<anonymous>)","at v._clearKernelState (c:\\Users\\hp\\.vscode\\extensions\\ms-toolsai.jupyter-2022.4.1000912146\\out\\extension.node.js:2:1218430)","at v.dispose (c:\\Users\\hp\\.vscode\\extensions\\ms-toolsai.jupyter-2022.4.1000912146\\out\\extension.node.js:2:1211912)","at c:\\Users\\hp\\.vscode\\extensions\\ms-toolsai.jupyter-2022.4.1000912146\\out\\extension.node.js:2:508868","at t.swallowExceptions (c:\\Users\\hp\\.vscode\\extensions\\ms-toolsai.jupyter-2022.4.1000912146\\out\\extension.node.js:2:900969)","at dispose (c:\\Users\\hp\\.vscode\\extensions\\ms-toolsai.jupyter-2022.4.1000912146\\out\\extension.node.js:2:508846)","at t.RawSession.dispose (c:\\Users\\hp\\.vscode\\extensions\\ms-toolsai.jupyter-2022.4.1000912146\\out\\extension.node.js:2:512523)","at runMicrotasks (<anonymous>)","at processTicksAndRejections (node:internal/process/task_queues:96:5)"]}],"source":["while True:\n","    success, img=capture.read()                       # captured image is saved in img\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)        # captured image is converted from BGR to RGB\n","    results = hands.process(img)                      # hands from captured image are recongnized and processed in results\n","          \n","    if results.multi_hand_landmarks:                  # more than one hands\n","        for handLms in results.multi_hand_landmarks:  # for each hand run the loop and access it information\n","            lmList = []                               # empty lsit that will contain list of sublist of [id, cx, cy]\n","           # mp_drawing.draw_landmarks(img, handLms, mp_hands.HAND_CONNECTIONS) #for each hand draw all 20 landmarks\n","            for id, lm in enumerate(handLms.landmark):\n","               #print(id, lm)                          #display id=landmarks, lm=landmarks coordinates for all 20 landmarks\n","               h, w, c=img.shape                       # height width channel(red green blue) of captured image\n","               cx, cy=int(lm.x*w), int(lm.y*h)         # cx= landmark_x_coordinated * widtH_of_the_captured_image \n","               #print(id, cx, cy)\n","               #print(id, h, w) \n","               lmList.append([id, cx, cy])\n","               #print(lmList)\n","            \n","            if lmList:\n","                xpos1, ypos1= lmList[4][1],lmList[4][2] #coordinates of thumb\n","                xpos2, ypos2= lmList[8][1],lmList[8][2] #coordinates of forefinger\n","                \n","                #draw circles around the thumb and forefinger\n","                cv2.circle(img,(xpos1, ypos1), 15, (255, 0, 0), cv2.FILLED)\n","                cv2.circle(img,(xpos2, ypos2), 15, (255, 0, 0), cv2.FILLED)\n","                cv2.line(img,(xpos1, ypos1),(xpos2, ypos2), (0, 255, 0), 6)\n","    \n","                length = math.hypot(xpos2-xpos1, ypos2-ypos1) #measure distance between thumb and forefinger\n","                #print(length)\n","                \n","                zpos1=(xpos1 + xpos2)//2\n","                zpos2=(ypos1 + ypos2)//2\n","                cv2.circle(img,(zpos1, zpos2), 15, (255, 0, 0), cv2.FILLED)\n","                if length<20:\n","                    cv2.circle(img,(zpos1, zpos2), 15, (0, 0, 255), cv2.FILLED)\n","            \n","            volRange = volume.GetVolumeRange()\n","            minVol = volRange[0]\n","            maxVol = volRange[1]\n","           \n","            # from numpy we find our length,by converting hand range in terms of volume range ie b/w -63.5 to 0\n","            vol = np.interp(length, [20, 250], [minVol, maxVol])\n","            volBar = np.interp(length, [20 ,250] , [400 ,150])\n","            volPer = np.interp(length, [20 ,250] , [0 ,100])\n","            volume.SetMasterVolumeLevel(vol, None)\n","            \n","            #volume bar for volume level\n","            cv2.rectangle(img , (50 ,150) , (85 , 400) ,(255, 0, 0) ,4)\n","            cv2.rectangle(img , (50 , int(volBar)) , (85 ,400) ,(255, 0, 0) ,cv2.FILLED)\n","            cv2.putText(img , str(int(volPer)) , (30, 450) ,cv2.FONT_HERSHEY_PLAIN ,4 , (0,0,255) , 3)\n","    \n","    cv2.imshow(\"Camera on\", img)\n","    cv2.waitKey(1)\n"," "]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":2}
